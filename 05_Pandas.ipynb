{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MMRES-PyBootcamp/MMRES-python-bootcamp2024/blob/master/05_Pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 5 - Pandas (Second part - 60')\n",
    "> An introduction on Pandas intermediate level concepts. Here we will present how to *manipulate* the data stored in a Pandas DataFrame, no matter if their Pandas Series store numerical, text or more complex data types. Finally we will introduce you some tools to *reshape*, *group* and *aggregate* their data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    " * [DataFrame transformations](#DataFrame-transformations)\n",
    "   * [DataFrame numerical transformations](#DataFrame-numerical-transformations)\n",
    "   * [DataFrame text transformations](#DataFrame-text-transformations)\n",
    "   * [Arbitrary transformations using `.apply()` method](#Arbitrary-transformations-using-.apply()-method) \n",
    " * [Exporting DataFrames](#Exporting-DataFrames)\n",
    " * [Grouping-by and aggregating DataFrames](#Grouping-by-and-aggregating-DataFrames)\n",
    " * [Pivoting DataFrames](#Pivoting-DataFrames)\n",
    " * [Melting DataFrames](#Melting-DataFrames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document is devised as a tool to enable your **self-learning process**. If you get stuck at some step or need any kind of help, please don't hesitate to raise your hand and ask for the teacher's guidance. Along it, you will find some **special cells**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Practice:</b> Practice cells announce exercises that you should try during the current boot camp session. Usually, solutions are provided using hidden cells (look for the dot dot dot symbol \"...\" and unravel it by clicking to check that your try is correct). \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Extension:</b> Extension cells correspond to exercises (or links to contents) that are a bit more advanced. We recommend to try them after the current boot camp session.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b> Tip cells just give some advice or complementary information.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"><b>Caveat:</b> Caveat cells warn you about the most common pitfalls one founds when starts his/her path learning Python.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame transformations\n",
    "\n",
    "We are now familiar on how to *access* the data stored in a DataFrame. Our next step will be how to *work* with such data. Let's begin again by loading Pandas with its typical `pd` alias and by importing `Spreadsheet.xlsx` straight from the [MMRES Python boot camp GitHub repository](https://github.com/MMRES-PyBootcamp/MMRES-python-bootcamp2023):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load package with its corresponding alias\n",
    "import pandas as pd\n",
    "\n",
    "# Define the GitHub url towards the SpreadSheet file in xlsx format\n",
    "url_excel = 'https://github.com/MMRES-PyBootcamp/MMRES-python-bootcamp2022/blob/main/datasets/'\n",
    "\n",
    "# Reading an Excel SpreadSheet and storing it as a DataFrame called `df`\n",
    "df = pd.read_excel(io=f'{url_excel}Spreadsheet.xlsx?raw=true')\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame numerical transformations\n",
    "\n",
    "For example, we could start by *standardizing* the values of a numerical column. By *standardizing* we mean taking a given distribution of values and bring it to a newer distribution with *mean equal zero* and *standard deviation equal one*. This *standardized* distribution is usually known as the [standard score](https://en.wikipedia.org/wiki/Standard_score) or *Z-score*. The $i$<sup>th</sup> observation of an $x$ magnitude, $(x_i)$, has a Z-score, $(Z_i)$, given by the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "Z_i = \\frac{x_i - \\mu(x)}{\\sigma(x)} ,\n",
    "\\end{equation}\n",
    "\n",
    "where, $\\mu(x)$ and $\\sigma(x)$ are the mean and the standard deviation of $x$, respectively. For example, let's get the Z-score of `'Intensity'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean of the 'Intensity': `I_mean`\n",
    "I_mean = df['Intensity'].mean()\n",
    "print(I_mean)\n",
    "\n",
    "# Get the standard deviation the 'Intensity': `I_std`\n",
    "I_std = df['Intensity'].std()\n",
    "print(I_std)\n",
    "\n",
    "# Computing the Z-score of the 'Intensity': `I_z`\n",
    "I_z = (df['Intensity'] - I_mean) / I_std\n",
    "\n",
    "# Storing `I_z` in as a new 'Z-Intensity' column inside `df`\n",
    "df['Z-Intensity'] = I_z\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how easy is:\n",
    "* To operate with a Pandas Series and numeric constants stored in variables: `(df['Intensity'] - I_mean) / I_std`.\n",
    "* To store a freshly created Series `I_z` into a preexisting DataFrame `df` with a new column name `'Z-Intensity'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Practice 1:</b>\n",
    "\n",
    "The $i$<sup>th</sup> observation of an $x$ magnitude, $(x_i)$, has a 0-to-1 normalization, $(N_i)$, given by the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "N_i = \\frac{x_i - m(x)}{M(x) - m(x)},\n",
    "\\end{equation}\n",
    "\n",
    "where, $m(x)$ and $M(x)$ are the minimum and the maximum values of $x$, respectively.\n",
    "    \n",
    "1) In the 1<sup>st</sup> code cell below, compute the 0-to-1 normalization of `'Amplitude'`. \n",
    "    \n",
    "Uncomment and fill only those code lines with underscores `___`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "ejercicio",
     "error"
    ]
   },
   "outputs": [],
   "source": [
    "# Get the minimum of the 'Amplitude': `A_min`\n",
    "#A_min = ___\n",
    "\n",
    "# Get the maximum of the 'Amplitude': `A_max`\n",
    "#A_max = ___\n",
    "\n",
    "# Print `A_min` and `A_max`\n",
    "#___\n",
    "#___\n",
    "\n",
    "# Compute the N-normalization of the 'Amplitude' and storing it in a new 'N-Amplitude' column\n",
    "#df['N-Amplitude'] = ___\n",
    "\n",
    "# Return the DataFrame\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "ejercicio",
     "solucion"
    ]
   },
   "outputs": [],
   "source": [
    "# Get the minimum of the 'Amplitude': `A_min`\n",
    "A_min = df['Amplitude'].min()\n",
    "\n",
    "# Get the maximum of the 'Amplitude': `A_max`\n",
    "A_max = df['Amplitude'].max()\n",
    "\n",
    "# Print `A_min` and `A_max`\n",
    "print(A_min)\n",
    "print(A_max)\n",
    "\n",
    "# Compute the N-normalization of the 'Amplitude' and storing it in a new 'N-Amplitude' column\n",
    "df['N-Amplitude'] = (df['Amplitude'] - A_min) / (A_max - A_min)\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Practice 1 ends here.</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's now devote some time in arranging `df` a bit more. For example, now that we have `'Z-Intensity'` and `'N-Amplitude'` we could discard `'Intensity'` and `'Amplitude'` using the [`.drop()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant columns 'Intensity' and 'Amplitude'\n",
    "list_drop = ['Intensity', 'Amplitude']\n",
    "df = df.drop(columns=list_drop)\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, is a good practice to use nice *self explanatory* labels for DataFrame columns. However, it is also recommended to use labels as *short* as possible (try to find your balance between self explanatory and short). With this in mind, let's update some column labels from `df` using the [`.rename()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.rename.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a renaming dictionary for incoming column rename\n",
    "dic_rename = {\n",
    "              # Key (Old name): Value (New name),\n",
    "              'Software': 'Soft',\n",
    "              'Sequence': 'Seq',\n",
    "              'Z-Intensity': 'I',\n",
    "              'N-Amplitude': 'A'\n",
    "             }\n",
    "\n",
    "# Rename some columns from `df`\n",
    "df = df.rename(columns=dic_rename)\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you remember that dictionaries were know as a *mapping data types*? When calling `df.rename(columns=dic_rename)`, we used `dic_rename` to *map* old column labels (*keys*) to new column labels (*values*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame text transformations\n",
    "\n",
    "Sometimes is useful to use text transformations on a given DataFrame column. For example, look at the column `'Raw'`. The strings within this column have a well organized structure comprising multiple substrings joined with underscores (`_`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return 'Raw' as a Series\n",
    "df['Raw']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we have a date (`1985-04-06`), a four-digit code (`0123`), a two-letters code (`GA`), some kind of single letter indicator (`T` / `C`), and another correlative indicator (`R1` / `R2` / `R3` / `R4`). Let's try extract this info and store it as new `df` columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split by underscore '_' the strings stored in 'Raw'\n",
    "df['Split raw'] = df['Raw'].str.split('_')\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we get a new column called `'Split raw'` that has lists within! To achieve this we first used the accessor method [`.str`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.html) to *access* the strings stored in `'Raw'`. Then, we *chained* the string method [`.split()`](https://docs.python.org/3/library/stdtypes.html#str.split) which, as you already know, returns a list. Now we should access the substrings stored within the lists stored in column `'Split raw'`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 1st element of the lists stored in 'Split raw' as 'Date'\n",
    "df['Date'] = df['Split raw'].str[0]\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this trick, we get a new column `'Date'` with the information we were looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Practice 2:</b>\n",
    "    \n",
    "1) In the 1<sup>st</sup> code cell below, get a new column called `'ID'` for the four-digit code (`0123`); a new column called `'User'` for the two-letters code (`GA`); a new column called `'Cond'` for the single letter indicator (`T` / `C`); and a new column called `'Rep'` for the correlative indicator (`R1` / `R2` / `R3` / `R4`).\n",
    "    \n",
    "2) In the 2<sup>nd</sup> code cell below, discard the columns `'Raw'` and `'Split raw'`.\n",
    "\n",
    "Uncomment and fill only those code lines with underscores `___`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "ejercicio"
    ]
   },
   "outputs": [],
   "source": [
    "# Take 2nd, 3rd, 4th, 5th elements of the lists stored in 'Split raw' as 'ID', 'User', 'Cond', 'Rep'\n",
    "#df['ID'] = ___\n",
    "#df['User'] = ___\n",
    "#df['Cond'] = ___\n",
    "#df['Rep'] = ___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "ejercicio",
     "solucion"
    ]
   },
   "outputs": [],
   "source": [
    "# Take 2nd, 3rd, 4th, 5th elements of the lists stored in 'Split raw' as 'ID', 'User', 'Cond', 'Rep'\n",
    "df['ID'] = df['Split raw'].str[1]\n",
    "df['User'] = df['Split raw'].str[2]\n",
    "df['Cond'] = df['Split raw'].str[3]\n",
    "df['Rep'] = df['Split raw'].str[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop redundant columns 'Raw' and 'Split raw'\n",
    "#list_drop = ___\n",
    "#df = df.drop(columns=___)\n",
    "\n",
    "# Return the DataFrame\n",
    "#___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop redundant columns 'Raw' and 'Split raw'\n",
    "list_drop = ['Raw', 'Split raw']\n",
    "df = df.drop(columns=list_drop)\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Practice 2 ends here.</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrary transformations using `.apply()` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we might need to use data manipulations more complex than the basic algebraic operations or string manipulations. For example, note the `'RNA'` column, could we *translate* the RNA sequences from this column as protein sequences? Of course we can! The only thing we need is to write a user defined function (UDF) implementing the rules to translate from RNA to protein:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to translate RNA to Protein\n",
    "def fun_RNA_to_Prot(RNAseq):\n",
    "    \"\"\"\n",
    "    Summary:\n",
    "        Translates RNA codon sequences into Protein amino acid sequences.\n",
    "    \n",
    "    Arguments:\n",
    "        RNAseq (string):\n",
    "            Input string with the RNA sequence\n",
    "    \"\"\" \n",
    "    \n",
    "    # Define dictionary with the rules to translate RNA codons into Protein amino acids\n",
    "    dict_RNA_to_Prot = {\"UUU\":\"F\", \"UUC\":\"F\", \"UUA\":\"L\", \"UUG\":\"L\", \"UCU\":\"S\", \"UCC\":\"s\",\n",
    "                        \"UCA\":\"S\", \"UCG\":\"S\", \"UAU\":\"Y\", \"UAC\":\"Y\", \"UAA\":\"STOP\", \"UAG\":\"STOP\",\n",
    "                        \"UGU\":\"C\", \"UGC\":\"C\", \"UGA\":\"STOP\", \"UGG\":\"W\", \"CUU\":\"L\", \"CUC\":\"L\",\n",
    "                        \"CUA\":\"L\", \"CUG\":\"L\", \"CCU\":\"P\", \"CCC\":\"P\", \"CCA\":\"P\", \"CCG\":\"P\",\n",
    "                        \"CAU\":\"H\", \"CAC\":\"H\", \"CAA\":\"Q\", \"CAG\":\"Q\", \"CGU\":\"R\", \"CGC\":\"R\",\n",
    "                        \"CGA\":\"R\", \"CGG\":\"R\", \"AUU\":\"I\", \"AUC\":\"I\", \"AUA\":\"I\", \"AUG\":\"M\",\n",
    "                        \"ACU\":\"T\", \"ACC\":\"T\", \"ACA\":\"T\", \"ACG\":\"T\", \"AAU\":\"N\", \"AAC\":\"N\",\n",
    "                        \"AAA\":\"K\", \"AAG\":\"K\", \"AGU\":\"S\", \"AGC\":\"S\", \"AGA\":\"R\", \"AGG\":\"R\",\n",
    "                        \"GUU\":\"V\", \"GUC\":\"V\", \"GUA\":\"V\", \"GUG\":\"V\", \"GCU\":\"A\", \"GCC\":\"A\",\n",
    "                        \"GCA\":\"A\", \"GCG\":\"A\", \"GAU\":\"D\", \"GAC\":\"D\", \"GAA\":\"E\", \"GAG\":\"E\",\n",
    "                        \"GGU\":\"G\", \"GGC\":\"G\", \"GGA\":\"G\", \"GGG\":\"G\"}\n",
    "    \n",
    "    # Initiate an empty string to be extended in the for loop below\n",
    "    Protseq = ''\n",
    "    \n",
    "    # Get range going from 0 to the length of the input RNAseq in steps of 3\n",
    "    # Remember that codons always comprise three nucleobases\n",
    "    for i in range(0, len(RNAseq), 3):\n",
    "        \n",
    "        # For each index in the range, slice the running RNAseq codon\n",
    "        codon = RNAseq[i:i+3]\n",
    "        \n",
    "        # Translate the running codon as an amino acid letter string\n",
    "        aa =  dict_RNA_to_Prot[codon]\n",
    "        \n",
    "        # Append the running amino acid letter string to our protein string\n",
    "        Protseq = Protseq + aa\n",
    "\n",
    "    # Return the protein sequence once translation is completed\n",
    "    return(Protseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry if you get a bit overwhelmed by this `fun_RNA_to_Prot()` at first glance. Just try to understand how does it works going from a its general structure to its details. Now you are in disposition to understand everything inside `fun_RNA_to_Prot()`, but take your time. Maybe, the most tricky part is the `range(0, len(RNAseq), 3)` thing. Have a look to the cell code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an RNA sequence to translate\n",
    "my_RNAseq = 'UGCGCCACG'\n",
    "\n",
    "# Get range going from 0 to the length of the input RNAseq (9) in steps of 3\n",
    "for i in range(0, len(my_RNAseq), 3):\n",
    "    \n",
    "    # Print the running index in the range\n",
    "    print(i)\n",
    "    \n",
    "    # Print the running RNAseq codon\n",
    "    print(my_RNAseq[i:i+3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b>\n",
    "    \n",
    "A good strategy to understand a complex Python code is trying to break it down into smaller pieces and run them separately. When dealing with for loops, I find useful to print the \"running\" variables in order to have a better intuition of what's going on in each loop.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test `fun_RNA_to_Prot()` with `my_RNAseq`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate `my_RNAseq` to protein\n",
    "fun_RNA_to_Prot(my_RNAseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have this cool `fun_RNA_to_Prot()` UDF, let's *apply* it to our `'RNA'` column. We have the [`.apply()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) method for this purpose, look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply fun_RNA_to_Prot to 'RNA' column and store the output as a new 'Prot' column\n",
    "df['Prot'] = df['RNA'].apply(fun_RNA_to_Prot)\n",
    "\n",
    "# Return the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Extension:</b>\n",
    "\n",
    "In the example above we used `.apply()` as a *Series method* but it can also be used as a *DataFrame method*. This is useful when the function that you need to apply involves multiple DataFrame columns. In such cases you need lo leverage [*lambda functions*](https://docs.python.org/3/reference/expressions.html#lambda) in conjunction with regular UDFs.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, `df` is clean enough as to be exported and locally stored. Look how easy is to save a DataFrame into our hard-disk with the method [`.to_excel()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.to_excel.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the DataFrame as an Excel SpreadSheet\n",
    "df.to_excel(excel_writer='datasets/DataFrame.xlsx', sheet_name='Excel_df', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping-by and aggregating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame method [`.groupby()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) is one of the most useful to start diving in your data. A group-by-and-aggregate operation takes place is three steps.\n",
    "\n",
    "1) DataFrame rows are **grouped by** the categories within a given column (or columns).\n",
    "2) The column (or columns) we want to aggregate are accessed.\n",
    "3) The accessed columns are then **aggregated** using an aggregating method.\n",
    "\n",
    "For example, suppose that we would like to know the mean `'I'` and `'A'` according to each `Soft`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Soft' and aggregate 'I', 'A' with mean\n",
    "df_g = df.groupby(by=['Soft'])[['I', 'A']].mean()\n",
    "\n",
    "# Return the DataFrame\n",
    "df_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, maybe we would like to know the mean `'I'` and `'A'` according to each `Node`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Node' and aggregate 'I', 'A' with mean\n",
    "df_g = df.groupby(by=['Node'])[['I', 'A']].mean()\n",
    "\n",
    "# Return the DataFrame\n",
    "df_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wen can also group-by multiple columns to have the information a bit more explicit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Soft', 'Node' and 'Prot' and aggregate 'I', 'A' with mean\n",
    "df_g = df.groupby(by=['Soft', 'Node', 'Prot'])[['I', 'A']].mean()\n",
    "\n",
    "# Return the DataFrame\n",
    "df_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that grouping by `'Prot'` is not really necessary with this DataFrame because we have a single protein sequence `'PEPTIDE'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Practice 3:</b>\n",
    "    \n",
    "1) In the 1<sup>st</sup> code cell below, group `df` by `'Soft'`, `'Node'`, `'Prot'`, and aggregate `'A'` with the minimum. Store the \"grouped-by-and-aggregated\" DataFrame as `df_g_Amin`.\n",
    "    \n",
    "2) In the 2<sup>nd</sup> code cell below, group `df` by `'Soft'`, `'Node'`, `'Prot'`, and aggregate `'I'` with the maximum. Store the \"grouped-by-and-aggregated\" DataFrame as `df_g_Imax`.\n",
    "    \n",
    "Un-comment and fill only those code lines with underscores `___`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Soft', 'Node' and 'Prot' and aggregate 'A' with min\n",
    "#df_g_Amin = ___\n",
    "\n",
    "# Return the DataFrame\n",
    "#___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by 'Soft', 'Node' and 'Prot' and aggregate 'A' with min\n",
    "df_g_Amin = df.groupby(by=['Soft', 'Node', 'Prot'])[['A']].min()\n",
    "\n",
    "# Return the DataFrame\n",
    "df_g_Amin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'Soft', 'Node' and 'Prot' and aggregate 'I' with max\n",
    "#df_g_Imax = ___\n",
    "\n",
    "# Return the DataFrame\n",
    "#___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by 'Soft', 'Node' and 'Prot' and aggregat 'I' with max\n",
    "df_g_Imax = df.groupby(by=['Soft', 'Node', 'Prot'])[['I']].max()\n",
    "\n",
    "# Return the DataFrame\n",
    "df_g_Imax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Practice 3 ends here.</b>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the `.groupby()` method on a DataFrame returns a *DataFrameGroupBy object* that has another method called [`.agg()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html). This method is useful when we want to use multiple aggregating functions at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of columns to group by with\n",
    "list_gby = ['Soft', 'Node', 'Prot']\n",
    "\n",
    "# Create a list of columns to aggregate\n",
    "list_agg = ['A', 'I']\n",
    "\n",
    "# Create a list with pandas string function aliases to aggregate with\n",
    "list_funs = ['min', 'max']\n",
    "\n",
    "# Group by and aggregate with multiple functions\n",
    "df_g = df.groupby(by=list_gby)[list_agg].agg(func=list_funs)\n",
    "\n",
    "# Return the DataFrame\n",
    "df_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we get the minimum and the maximum for both columns `I` and `A`. Since we just want maximum `I` and minimum `A`, it would be great to specify which aggregating functions we want for each column. We can achieve this with a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary specifying how to aggregate each column\n",
    "dict_aggfuns = {'A': 'min', 'I': 'max'}\n",
    "\n",
    "# Group by and aggregate specifying how to aggregate each column\n",
    "df_g = df.groupby(by=list_gby).agg(func=dict_aggfuns)\n",
    "\n",
    "# Return the DataFrame\n",
    "df_g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Extension:</b>\n",
    "\n",
    "We can specify the aggregating functions passed to the `.agg()` method as:\n",
    "1) Lists of builtin functions (like `min`, `max`).\n",
    "2) Lists of functions from packages (like `np.min`, `np.max`).\n",
    "3) Lists of \"[Pandas string aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#the-aggregate-method)\" for functions (like `'min'`, `'max'`).\n",
    "\n",
    "There are a sundry of [Pandas built-in aggregation methods](https://pandas.pydata.org/pandas-docs/stable/user_guide/groupby.html#built-in-aggregation-methods). The possibilities are almost endless.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are an experienced spreadsheet user, maybe you will find more familiar the term \"pivot table\" rather than \"grouping-by and aggregating\". In general, all that can be achieve by grouping-by-and-aggregating can also be done with the [`.pivot_table()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.pivot_table.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by and aggregate with multiple functions\n",
    "df.groupby(by=list_gby).agg(func=dict_aggfuns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot and aggregate with multiple functions\n",
    "df.pivot_table(index=list_gby, aggfunc=dict_aggfuns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the perfect correspondence between the `by=` parameter from `.groupby()` and the `index=` parameter from `.pivot_table()`, and similarly, between the `func=` parameter from the `.groupby()` method `.agg()` and the `aggfunc=` parameter from `.pivot_table()`. One distinguishing feature of `.pivot_table()` is the parameter `columns=`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of columns to be pivot indexes\n",
    "list_indexes = ['Prot']\n",
    "\n",
    "# Create a list of columns to be pivot columns\n",
    "list_columns = ['Soft', 'Node']\n",
    "\n",
    "# Pivot data and return the corresponding DataFrame\n",
    "df.pivot_table(index=list_indexes, columns=list_columns, aggfunc=dict_aggfuns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By specifying `columns=`, we can now split `'Soft'` and `'Node'` categories as separated columns in the output pivot table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Melting DataFrames\n",
    "\n",
    "In a \"[Tidy DataFrame](https://www.jstatsoft.org/article/view/v059i10)\", each variable is a column and each observation is a row. The Pandas function [`melt()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.melt.html) allows to switch from a \"Non-tidy DataFrame\" to a \"Tidy DataFrame\" very easily. Since our example DataFrame `df` is quite tidy, let's rename columns `'I'` and `'A'` just to better illustrate how does `melt()` works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a renaming dictionary for incoming column rename\n",
    "dic_rename = {\n",
    "              # Key (Old name); Value (New name),\n",
    "              'I': 'Cat',\n",
    "              'A': 'Dog'\n",
    "             }\n",
    "\n",
    "# Rename some columns from `df`\n",
    "df_trick = df.rename(columns=dic_rename)\n",
    "\n",
    "# Return the DataFrame (BEFORE melting)\n",
    "df_trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have rows that mix observations for `'Cat'` and `'Dog'`. Let's melt this \"Non-tidy DataFrame\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt 'Cat' and 'Dog', keeping 'Soft', 'Node', 'Prot', 'Cond' and 'Rep'\n",
    "df_melt = pd.melt(\n",
    "                  frame=df_trick,\n",
    "                  id_vars=['Soft', 'Node', 'Prot', 'Cond', 'Rep'],\n",
    "                  value_vars=['Cat', 'Dog'],\n",
    "                  var_name='Animal',\n",
    "                  value_name='Score',\n",
    "                  )\n",
    "\n",
    "# Return the DataFrame (AFTER melting)\n",
    "df_melt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in `df_melt`, each row is an observation and each column is a variable. Note the arguments we used in `pd.melt()`:\n",
    " + `id_vars=`: List of columns to use as identifiers on the \"melted\" DataFrame.\n",
    " + `value_vars=`: List of columns to \"melt\".\n",
    " + `var_name=`: String to name the column with the \"melted\" column names.\n",
    " + `value_name=`: String to name the column with the \"melted\" column values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b>\n",
    "    \n",
    "Despite pivot tables are easier to inspect at a glance than \"Tidy DataFrames\", it is always recommended to work with *tidy data*. In the boot camp session that we will devote to data visualization on September 21 <sup>st</sup> (11:00-12:00), we will see that many Python plotting functions work better with \"Tidy DataFrames\".\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "04_Pandas.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
